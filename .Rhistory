?gsub
?space
str_replace_all(x, space(), "")
library(stringr)
?space
str_replace_all(x, space(), "")
?str_replace_all
?`[[:punct::]`
stopwords <-  function(language){
if (language == 'english'){
return(c('and', 'or', 'if', 'i', 'in', 'an'))
}
if (language == 'french'){
return(c('et', 'je'))
}
}
clean_lower <- function(text_vector){
tolower(text_vector)
}
clean_whitespace <- function(text_vector){
str_replace_all(text_vector, '//s', ' ')
}
clean_numbers <- function(text_vector){
str_repace_all(text_vector,  '[0-9]', '')
}
clean_punctuation <- function(text_vector){
str_repace_all(text_vector, '[[:punct:]]', '')
}
clean_stopwords <- function(text_vector, stopwords = stopwords('english')){
str_repace_all(text_vector, stopwords, '')
}
text_data %>%
clean_lower %>%
clean_whitespace %>%
clean_numbers %>%
clean_punctuation %>%
clean_stopwords %>%
tokenise
test_data <- c('an example of text',
'more text input you might have',
'text!!!',
'2 sentences of text. In this document')
test_data2 <- as.factor(test_data)
test_data %>%
clean_lower %>%
clean_whitespace %>%
clean_numbers %>%
clean_punctuation %>%
clean_stopwords %>%
tokenise
library(stringr)
###################### Cleaning ####################
stopwords <-  function(language){
if (language == 'english'){
return(c('and', 'or', 'if', 'i', 'in', 'an'))
}
if (language == 'french'){
return(c('et', 'je'))
}
}
clean_lower <- function(text_vector){
tolower(text_vector)
}
clean_whitespace <- function(text_vector){
str_replace_all(text_vector, '//s', ' ')
}
clean_numbers <- function(text_vector){
str_replace_all(text_vector,  '[0-9]', '')
}
clean_punctuation <- function(text_vector){
str_replace_all(text_vector, '[[:punct:]]', '')
}
clean_stopwords <- function(text_vector, stopwords = stopwords('english')){
str_replace_all(text_vector, stopwords, '')
}
############### Tokenise ##################################
tokenise <- function(text_vector, method = 'remove_punctuation'){
# Turn text into word tokens
# Make sure this is character, not factor
text_vector <- as.character(text_vector)
# Tokenise, ignoring punctuation
if (method == 'remove_punctuation'){
return(str_split(text_vector, boundary('word')))
}
# Tokenise, keeping punctuation associated with word
if (method == 'keep_punctuation'){
return(str_split(text_vector, ' '))
}
}
################# Word count ##########################
############## Term Document Matrix ######################
#########
# Tests #
#########
test_data <- c('an example of text',
'more text input you might have',
'text!!!',
'2 sentences of text. In this document')
test_data2 <- as.factor(test_data)
test_data %>%
clean_lower %>%
clean_whitespace %>%
clean_numbers %>%
clean_punctuation %>%
clean_stopwords %>%
tokenise
tokenise(test_data, method = 'keep_punctuation')
tokenise(test_data2)
test_data %>%
clean_lower %>%
clean_whitespace %>%
clean_numbers %>%
clean_punctuation %>%
clean_stopwords %>%
tokenise
test_data2 <- as.factor(test_data)
library(stringr)
test_data %>%
clean_lower %>%
clean_whitespace %>%
clean_numbers %>%
clean_punctuation %>%
clean_stopwords %>%
tokenise
library(stringr)
###################### Cleaning ####################
stopwords <-  function(language){
if (language == 'english'){
return(c('and', 'or', 'if', 'i', 'in', 'an'))
}
if (language == 'french'){
return(c('et', 'je'))
}
}
clean_lower <- function(text_vector){
tolower(text_vector)
}
clean_whitespace <- function(text_vector){
str_replace_all(text_vector, '//s', ' ')
}
clean_numbers <- function(text_vector){
str_replace_all(text_vector,  '[0-9]', '')
}
clean_punctuation <- function(text_vector){
str_replace_all(text_vector, '[[:punct:]]', '')
}
clean_stopwords <- function(text_vector, stopwords = stopwords('english')){
str_replace_all(text_vector, stopwords, '')
}
############### Tokenise ##################################
tokenise <- function(text_vector, method = 'remove_punctuation'){
# Turn text into word tokens
# Make sure this is character, not factor
text_vector <- as.character(text_vector)
# Tokenise, ignoring punctuation
if (method == 'remove_punctuation'){
return(str_split(text_vector, boundary('word')))
}
# Tokenise, keeping punctuation associated with word
if (method == 'keep_punctuation'){
return(str_split(text_vector, ' '))
}
}
################# Word count ##########################
############## Term Document Matrix ######################
#########
# Tests #
#########
test_data <- c('an example of text',
'more text input you might have',
'text!!!',
'2 sentences of text. In this document')
test_data2 <- as.factor(test_data)
test_data %>%
clean_lower %>%
clean_whitespace %>%
clean_numbers %>%
clean_punctuation %>%
clean_stopwords %>%
tokenise
test_data %>%
clean_lower %>%
clean_whitespace %>%
clean_numbers %>%
clean_punctuation %>%
clean_stopwords %>%
tokenise
tokenise(test_data, method = 'keep_punctuation')
tokenise(test_data2)
test_data %>%
clean_lower %>%
clean_whitespace %>%
clean_numbers %>%
clean_punctuation %>%
clean_stopwords %>%
tokenise
stopwords('english')
clean_stopwords <- function(text_vector, stopwords = stopwords('english')){
str_replace_all(text_vector, stopwords, '')
}
test_data %>%
clean_lower %>%
clean_whitespace %>%
clean_numbers %>%
clean_punctuation %>%
clean_stopwords %>%
tokenise
test_data %>%
clean_lower %>%
clean_whitespace %>%
clean_numbers %>%
clean_punctuation %>%
clean_stopwords %>%
tokenise
