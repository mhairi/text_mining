{
    "contents" : "library(stringr)\nlibrary(dplyr)\nlibrary(slam)\n\n###################### Cleaning ####################\n\nstopwords <-  function(language){\n  if (language == 'english'){\n    return(c('and', 'or', 'if', 'i', 'in', 'an'))  \n  }\n  if (language == 'french'){\n    return(c('et', 'je')) \n  }\n}\n\nclean_lower <- function(text_vector){\n  tolower(text_vector)\n}\n\nclean_whitespace <- function(text_vector){\n str_replace_all(text_vector, '//s', ' ')\n}\n\nclean_numbers <- function(text_vector){\n str_replace_all(text_vector,  '[0-9]', '')\n}\n  \nclean_punctuation <- function(text_vector){\n str_replace_all(text_vector, '[[:punct:]]', '') \n}\n\nclean_stopwords <- function(text_vector, words = stopwords('english')){\n  suppressWarnings(str_replace_all(text_vector, words, ''))\n  # Warns about recycling the final argument of str_replace_all\n}\n\n############### Tokenise ##################################\n  \n\ntokenise <- function(text_vector, method = 'remove_punctuation'){\n  # Turn text into word tokens\n  \n  # Make sure this is character, not factor\n  text_vector <- as.character(text_vector)\n  \n  # Tokenise, ignoring punctuation\n  if (method == 'remove_punctuation'){\n   return(str_split(text_vector, boundary('word')))\n  }\n  \n  # Tokenise, keeping punctuation associated with word\n  if (method == 'keep_punctuation'){\n   return(str_split(text_vector, ' ')) \n  }\n}\n\n\n\n################# Word count ##########################\n\n\nword_count <- function(tokens){\ndata.frame(words = tokens %>% unlist) %>%\n  group_by(words) %>%\n  summarise(count = n()) %>%\n  arrange(-count)\n}\n\n\n\n############## Term Document Matrix ######################\n\nterm_document_matrix <- function(tokens){\n \n  # What are the unique documents?\n  documents <- as.character(1:length(tokens))\n  \n  # What are the unique terms?\n  terms <- tokens %>% unlist %>% unique %>% sort\n  \n  # How big does this matrix need to be\n  ndocs <-  length(documents)\n  nterms <- length(terms)\n  \n  # Build up i, j and v for each document\n  i <- numeric(0)\n  j <- numeric(0)\n  v <- numeric(0)\n  \n  for (doc in seq_along(tokens)){\n    # Word count within a document\n    temp_df <- word_count(tokens[doc])\n    \n    # Get values of j, i\n    temp_i <- match(temp_df$words, terms)\n    temp_j <- rep(doc, nrow(temp_df))\n    temp_v <- temp_df$count\n    \n    # Add to full list  \n    i <- c(i, temp_i)\n    j <- c(j, temp_j)\n    v <- c(v, temp_v)\n  }\n  \n  return( \n  simple_triplet_matrix(i, j, v,\n                        nrow     = nterms,\n                        ncol     = ndocs,\n                        dimnames = list(terms     = terms,\n                                        documents = documents))\n  )\n}\n\n\n##############\n# Trying out #\n##############\n\ntest_data <- c('an example of text',\n               'more text input you might have',\n               'text!!!',\n               '2 sentences of text. In this document')\n\ntest_data2 <- as.factor(test_data)\n\ntest_data %>%\n  clean_lower %>%\n  clean_whitespace %>%\n  clean_numbers %>%\n  clean_punctuation %>%\n  clean_stopwords %>%\n  tokenise\n\ntest_tokens <- \ntest_data %>%\n  clean_lower %>%\n  clean_whitespace %>%\n  clean_numbers %>%\n  clean_punctuation %>%\n  clean_stopwords %>%\n  tokenise\n\ntokenise(test_data, method = 'keep_punctuation')\ntokenise(test_data2)\n\nword_count(test_tokens)\nword_count(test_data %>% tokenise)\n\nterm_document_matrix(test_tokens) %>% as.matrix\n\n######## Larger Data ###############\n\ntest_data3 <- read.csv('reviews.csv')\n\ntokens <-\ntest_data3$text %>%\n  clean_lower %>%\n  clean_whitespace %>%\n  clean_punctuation %>%\n  clean_stopwords %>%\n  tokenise\n\ntokens %>% word_count\n\ntokens %>% term_document_matrix\n  \n  ",
    "created" : 1435997821298.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1622696204",
    "id" : "91C70871",
    "lastKnownWriteTime" : 1437380600,
    "path" : "~/RPackages/tm2/R/tm2.R",
    "project_path" : "R/tm2.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 0,
    "source_on_save" : false,
    "type" : "r_source"
}